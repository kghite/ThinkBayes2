{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Bayesian Report 1\n",
    "---\n",
    "This notebook provides descriptions and solutions for several interesting problems that can be solved utilizing Bayes' Theorem.  The first six problems (with the exception of a few story changes) are from *My Favorite Bayes's Theorem Problems*, by Allen Downey, which can be found here: http://allendowney.blogspot.com/2011/10/my-favorite-bayess-theorem-problems.html.  The inspiration for Problem 7 can be found at , and Problem 8 details an original problem and solution also using Bayes' Theorem.\n",
    "\n",
    "Kathryn Hite 9/12/16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Background\n",
    "### Bayes' Theorem\n",
    "Bayes' Theorem allows us to compute the probability of event $A$ occurring given that event $B$ has occurred using the following equation:\n",
    "\n",
    "$p(A|B) = \\frac{p(A)p(B|A)}{p(B)}$\n",
    "\n",
    "This is derived from conjunctive probability, which states that \n",
    "\n",
    "$p(A and B) = p(A)p(B|A)$\n",
    "\n",
    "when the probability of $B$ is dependent on the outcome of $A$.  Because conjunction is communative, $A$ and $B$ can be switched to create the expression\n",
    "\n",
    "$p(B and A) = p(B)p(A|B)$.\n",
    "\n",
    "By setting these two values equal to each other we get the rearranged version of Bayes' Theorem\n",
    "\n",
    "$p(A)p(B|A) = p(B)p(A|B)$\n",
    "\n",
    "### The Diachronic Interpretation\n",
    "The diachronic interpretation of Bayes' Theorem allows us to determine the likelihood of a hypothesis given some set of data.  Given a hypothesis $H$ and data $D$, we can write probability of $H$ given $D$ as \n",
    "\n",
    "$p(H|D) = \\frac{p(H)p(D|H)}{p(D)}$,\n",
    "\n",
    "where\n",
    "\n",
    "- $p(H)$ is the Prior,\n",
    "- $p(H|D)$ is the Posterior,\n",
    "- $p(D|H)$ is the Likelihood,\n",
    "- and $p(D)$ is the Normalizing Constant.\n",
    "\n",
    "The following problems utilize the diachronic interpretation of Bayes' Theorem to solve some (mostly) practical problems by determining the likelihood of some case given an action or observation as our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1\n",
    "### The Cookie Problem\n",
    "\n",
    "**Problem:** In this problem, we consider two bowls of cookies.  Bowl 1 contains 10 chocolate chip cookies and 30 plain cookies, while bowl 2 contains 20 chocolate chip and 20 plain cookies.  If we were to choose a plain cookie from a random bowl, what is the probability that the cookie was taken from bowl 1?  (Note: To successfully recreate this problem on your own, you must remember to check the type of cookie taken from the bowl *before* eating it.)\n",
    "\n",
    "**Solution:** This problem is easily solved using the table method of representing the diachronic interpretation of Bayes' Theorem.  Our first hypothesis $H_1$ is that we drew a cookie from Bowl 1.  We compare this to the $H_2$, which must be the alternative: that we drew a cookie from Bowl 2.  The problem states that the cookie that we drew is plain, so the data $D$ is that the cookie is plain.  These values allow us to construct the following table to compute \n",
    "\n",
    "$p(H|D) = \\frac{p(H)p(D|H)}{p(D)}$\n",
    "\n",
    "for $H_1$ and $H_2$.\n",
    "\n",
    "| **Hypothesis** |  **Prior Prob** | **Likelihood** | **Unnormalized Posterior** | **Posterior** |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| $H$ | $p(H)$ | $p(D|H)$ | $p(H)p(D|H)$ | $\\frac{p(H)p(D|H)}{p(D)}$ |\n",
    "| In words: | p(Choosing the bowl given no cookie data) | p(Pulling a plain cookie from this bowl) | Prior * Likelihood | p(Getting plain cookie from this bowl) |\n",
    "| Bowl 1 | $\\frac{1}{2}$ | $\\frac{3}{4}$ | $\\frac{3}{8}$ | $\\frac{3}{5}$ |\n",
    "| Bowl 2 | $\\frac{1}{2}$ | $\\frac{1}{2}$ | $\\frac{1}{4}$ | $\\frac{2}{5}$ |\n",
    "| | | | **Total Probability** | |\n",
    "| | | | $p(D)$ | |\n",
    "| | | | $\\frac{5}{8}$ | |\n",
    "\n",
    "Based on the table, we see that the probability of the cookie having come from Bowl 1 is equal to $p(H_1|D)$ or $\\frac{3}{5}$.  This is a great example of a very straightforward problem where we can easily determine what numbers should be used to quickly fill in the table model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2\n",
    "### The M&M Problem\n",
    "Now that we've completed the cookie problem, how could we use Bayes' Theorem to think about problems with more than one piece of data?\n",
    "\n",
    "**Problem:** In 1994 and earlier, bags of M&Ms came with a different ratio of colors than they do today.  A detailed table of what colors you can expect from your M&M bags is shown below:\n",
    "\n",
    "| 1994 Bag | 30% Brown | 20% Yellow | 20% Red | 10% Green | 10% Orange | 10% Tan |\n",
    "| 2016 Bag | 13% Brown | 14% Yellow | 13% Red | 20% Green | 16% Orange | 24% Blue |\n",
    "\n",
    "Let's say that we have two bags of M&Ms, but one is from 1994 and one is from 2016.  We draw an M&M from each bag without looking at the bags and end up with a green M&M and a yellow M&M.  Obviously eating an M&M from 1994 could be hazardous to your health, so we should reasonably sure of which bag the M&M that we plan on eating is coming from.  What is the probability that the yellow M&M came from the 1994 bag?\n",
    "\n",
    "**Solution:** We can build on to the table used in Problem 1 by updating the values and considering how to incorporate both pieces of data that we currently have.\n",
    "\n",
    "| **Hypothesis** |  **Prior Prob** | **Likelihood** | **Unnormalized Posterior** | **Posterior** |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| $H$ | $p(H)$ | $p(D|H)$ | $p(H)p(D|H)$ | $\\frac{p(H)p(D|H)}{p(D)}$ |\n",
    "| 1994 Bag | $\\frac{1}{2}$ | $(20)(20)$ | $200$ | $\\frac{20}{27}$ |\n",
    "| 2016 Bag | $\\frac{1}{2}$ | $(14)(10)$ | $70$ | $\\frac{7}{27}$ |\n",
    "| | | | **Total Probability** | |\n",
    "| | | | $p(D)$ | |\n",
    "| | | | $27$ | |\n",
    "\n",
    "Our Prior Probability is still $\\frac{1}{2}$ given no data about the bags, but notice that we can integrate both pieces of M&M data into the Likelihood simultaneously.  The probability of getting a yellow M&M from the 1994 bag and a green from the 2016 given that the two actions are completely independent is \n",
    "\n",
    "$p(A and B) = p(A)p(B) = (20\\%)(20\\%).\n",
    "\n",
    "The opposite case is considered for the second hypothesis, and we get the following answer: \n",
    "\n",
    "There is a $\\frac{20}{27}$ that the yellow M&M was taken from the 1994 bag.  Given those odds, I would definitely recommend eating the green one, but you never know.  Good luck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3\n",
    "### The Elvis Twin Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4\n",
    "### The Crime Scene Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 5\n",
    "### The Smoking Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 6\n",
    "### The Monty Hall Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 7\n",
    "### New Problem"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
