{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Bayesian Report 1\n",
    "---\n",
    "This notebook provides descriptions and solutions for several interesting problems that can be solved utilizing Bayes' Theorem.  The first four problems (with the exception of a few story changes) are excerpts from *My Favorite Bayes's Theorem Problems*, by Allen Downey, which can be found here: http://allendowney.blogspot.com/2011/10/my-favorite-bayess-theorem-problems.html.  The inspiration for Problem 5 can be found at https://www.reddit.com/r/probabilitytheory/comments/500vjf/what_is_the_logic_behind_solving_this_question/, and Problem 6 details an original robotics problem and solution also using Bayes' Theorem.\n",
    "\n",
    "Kathryn Hite 9/12/16\n",
    "License: Attribution 4.0 International (CC BY 4.0) \n",
    "https://creativecommons.org/licenses/by/4.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Background\n",
    "### Bayes' Theorem\n",
    "Bayes' Theorem allows us to compute the probability of event $A$ occurring given that event $B$ has occurred using the following equation:\n",
    "\n",
    "$p(A|B) = \\frac{p(A)p(B|A)}{p(B)}$\n",
    "\n",
    "This is derived from conjunctive probability, which states that \n",
    "\n",
    "$p(A and B) = p(A)p(B|A)$\n",
    "\n",
    "when the probability of $B$ is dependent on the outcome of $A$.  Because conjunction is communative, $A$ and $B$ can be switched to create the expression\n",
    "\n",
    "$p(B and A) = p(B)p(A|B)$.\n",
    "\n",
    "By setting these two values equal to each other we get the rearranged version of Bayes' Theorem\n",
    "\n",
    "$p(A)p(B|A) = p(B)p(A|B)$\n",
    "\n",
    "### The Diachronic Interpretation\n",
    "The diachronic interpretation of Bayes' Theorem allows us to determine the likelihood of a hypothesis given some set of data.  Given a hypothesis $H$ and data $D$, we can write probability of $H$ given $D$ as \n",
    "\n",
    "$p(H|D) = \\frac{p(H)p(D|H)}{p(D)}$,\n",
    "\n",
    "where\n",
    "\n",
    "- $p(H)$ is the Prior,\n",
    "- $p(H|D)$ is the Posterior,\n",
    "- $p(D|H)$ is the Likelihood,\n",
    "- and $p(D)$ is the Normalizing Constant.\n",
    "\n",
    "The following problems utilize the diachronic interpretation of Bayes' Theorem to solve some (mostly) practical problems by determining the likelihood of some case given an action or observation as our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1\n",
    "### The Cookie Problem\n",
    "\n",
    "**Problem:** In this problem, we consider two bowls of cookies.  Bowl 1 contains 10 chocolate chip cookies and 30 plain cookies, while bowl 2 contains 20 chocolate chip and 20 plain cookies.  If we were to choose a plain cookie from a random bowl, what is the probability that the cookie was taken from bowl 1?  (Note: To successfully recreate this problem on your own, you must remember to check the type of cookie taken from the bowl *before* eating it.)\n",
    "\n",
    "**Solution:** This problem is easily solved using the table method of representing the diachronic interpretation of Bayes' Theorem.  Our first hypothesis $H_1$ is that we drew a cookie from Bowl 1.  We compare this to the $H_2$, which must be the alternative: that we drew a cookie from Bowl 2.  The problem states that the cookie that we drew is plain, so the data $D$ is that the cookie is plain.  These values allow us to construct the following table to compute \n",
    "\n",
    "$p(H|D) = \\frac{p(H)p(D|H)}{p(D)}$\n",
    "\n",
    "for $H_1$ and $H_2$.\n",
    "\n",
    "| **Hypothesis** |  **Prior Prob** | **Likelihood** | **Unnormalized Posterior** | **Posterior** |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| $H$ | $p(H)$ | $p(D|H)$ | $p(H)p(D|H)$ | $\\frac{p(H)p(D|H)}{p(D)}$ |\n",
    "| In words: | p(Choosing the bowl given no cookie data) | p(Pulling a plain cookie from this bowl) | Prior * Likelihood | p(Getting plain cookie from this bowl) |\n",
    "| Bowl 1 | $\\frac{1}{2}$ | $\\frac{3}{4}$ | $\\frac{3}{8}$ | $\\frac{3}{5}$ |\n",
    "| Bowl 2 | $\\frac{1}{2}$ | $\\frac{1}{2}$ | $\\frac{1}{4}$ | $\\frac{2}{5}$ |\n",
    "| | | | **Total Probability** | |\n",
    "| | | | $p(D)$ | |\n",
    "| | | | $\\frac{5}{8}$ | |\n",
    "\n",
    "Based on the table, we see that the probability of the cookie having come from Bowl 1 is equal to $p(H_1|D)$ or $\\frac{3}{5}$.  This is a great example of a very straightforward problem where we can easily determine what numbers should be used to quickly fill in the table model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2\n",
    "### The M&M Problem\n",
    "Now that we've completed the cookie problem, how could we use Bayes' Theorem to think about problems with more than one piece of data?\n",
    "\n",
    "**Problem:** In 1994 and earlier, bags of M&Ms came with a different ratio of colors than they do today.  A detailed table of what colors you can expect from your M&M bags is shown below:\n",
    "\n",
    "| **1994 Bag** | **2016 Bag** |\n",
    "| :----------: | :----------: |\n",
    "| 30% Brown | 13% Brown |\n",
    "| 20% Yellow | 14% Yellow |\n",
    "| 20% Red | 13% Red |\n",
    "| 10% Green | 20% Green |\n",
    "| 10% Orange | 16% Orange |\n",
    "| 10% Tan | 24% Blue |\n",
    "\n",
    "Let's say that we have two bags of M&Ms, but one is from 1994 and one is from 2016.  We draw an M&M from each bag without looking at the bags and end up with a green M&M and a yellow M&M.  Obviously eating an M&M from 1994 could be hazardous to your health, so we should reasonably sure of which bag the M&M that we plan on eating is coming from.  What is the probability that the yellow M&M came from the 1994 bag?\n",
    "\n",
    "**Solution:** We can build on to the table used in Problem 1 by updating the values and considering how to incorporate both pieces of data that we currently have.  Given the above problem statement we can determine:\n",
    "\n",
    "* $H_1$: Yellow M&M from the 1994 bag\n",
    "* $H_2$: Yellow M&M from the 2016 bag\n",
    "* $D$: Yellow and green M&M\n",
    "\n",
    "We can then fill in our table.\n",
    "\n",
    "| **Hypothesis** |  **Prior Prob** | **Likelihood** | **Unnormalized Posterior** | **Posterior** |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| $H$ | $p(H)$ | $p(D|H)$ | $p(H)p(D|H)$ | $\\frac{p(H)p(D|H)}{p(D)}$ |\n",
    "| 1994 Bag | $\\frac{1}{2}$ | $(20)(20)$ | $200$ | $\\frac{20}{27}$ |\n",
    "| 2016 Bag | $\\frac{1}{2}$ | $(14)(10)$ | $70$ | $\\frac{7}{27}$ |\n",
    "| | | | **Total Probability** | |\n",
    "| | | | $p(D)$ | |\n",
    "| | | | $27$ | |\n",
    "\n",
    "Our Prior Probability is still $\\frac{1}{2}$ given no data about the bags, but notice that we can integrate both pieces of M&M data into the Likelihood simultaneously.  The probability of getting a yellow M&M from the 1994 bag and a green from the 2016 given that the two actions are completely independent is \n",
    "\n",
    "$p(A and B) = p(A)p(B) = (20\\%)(20\\%).\n",
    "\n",
    "The opposite case is considered for the second hypothesis, and we get the following answer: \n",
    "\n",
    "There is a $\\frac{20}{27}$ that the yellow M&M was taken from the 1994 bag.  Given those odds, I would definitely recommend eating the green one, but you never know.  Good luck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3\n",
    "### The Smoking Problem\n",
    "\n",
    "Another available tool for solving Bayes' Theorem problems is known as the tree method.  This involves listing out all of the hypothetical options with their respective probabilites and constructing a final addition of all of the branch values down to our given hypothesis.  The following problem also requires decising what information is or is not important, which is more common than say the simple cookie problem, which tells you almost exactly how to proceed.\n",
    "\n",
    "**Problem:** Given the fact that women who smoke are 13 times more likely to develop lung cancer than women who don't, what is the probability that a woman smokes given that she has lung cancer?  \n",
    "\n",
    "**Solution:** As in the previous problems, we begin by identifying our hypotheses and data:\n",
    "\n",
    "* $H_1$: Woman who smokes\n",
    "* $H_2$: Woman who does not smoke\n",
    "* $D$: Woman has lung cancer\n",
    "\n",
    "We can construct the following tree of all of the possible hypotheses:\n",
    "\n",
    "![title](CancerTree.png)\n",
    "\n",
    "By following the branches down to smokers with cancer we get the number of women who smoke (numerator) out of the number of women who have lung cancer (denominator).\n",
    "\n",
    "$p(smoker with cancer) = \\frac{13xy}{13xy + x(1-y)}$\n",
    "\n",
    "We see that the x terms cancel out from the equation, and referencing the tree tells us that we don't have to worry about the value for how many women total are smokers.  The y term, however, is required, and given that $17.9\\%$ of women smoke (CDC, 2009), we can solve for our total probability.\n",
    "\n",
    "$p(smoker with cancer) = \\frac{13y}{13y + (1-y)} = \\frac{(13)(17.9)}{(13)(17.9) + (1-17.9)} = 74\\%$\n",
    "\n",
    "In this case, using the tree method helped us to determine what data would and would not be necessary, which can be useful for problems that have missing or additional information defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4\n",
    "### The Monty Hall Problem\n",
    "\n",
    "The Monty Hall problem describes your odds of winning a classic game show.  Let's utilize some code to solve it more quickly than with the table method.\n",
    "\n",
    "**Problem:** There are three closed doors.  Out of doors A, B, and C, one has a car behind it while the other two have goats.  You choose A, and the host reveals a goat behind door B.  You then have the opportunity to either keep door A or switch to the remaining door.  Whichever door you choose will be opened to reveal your prize.  If the host chose to open door B with a probability of $p$, what is the probabillity as a function of $p$ that the car is behind door A?\n",
    "\n",
    "**Solution:** As usual, let's begin with what we know:\n",
    "\n",
    "* $H_1$: Door A holds the car\n",
    "* $H_2$: Door B holds the car\n",
    "* $H_3$: Door C holds the car\n",
    "* $D$: The host picks door B\n",
    "\n",
    "We first assume that each door has an equal proability of holding the car at the beginning, so our Prior for each hypothesis is $\\frac{1}{3}$.  Now let's consider each case in order.  If the car is behind door A (which you have already chosen), the host can open either door B or door C and safely reveal a goat.  Thus B will be chosen with a probability of $p$.  The likelihood of the data is $0$ however if the car is behind door B, because the host would not have chosen that door.  And finally, if the car is behind door C and you chose door A, the host would have no other option but to choose door B.  Thus the likelihood of the data is $1$.\n",
    "\n",
    "Now that we have our priors and likelihoods.  We could construct a table, but let's try a faster method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import our Python framework\n",
    "from __future__ import print_function, division\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "from thinkbayes2 import Pmf, Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the thinkbayes2 package, we can import the Suite class that provides a framework for quick Bayesian updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 0.3333333333333333\n",
      "B 0.0\n",
      "C 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# We first create a class to encapsulate the likelihoods determined above\n",
    "class Monty(Suite):\n",
    "\n",
    "    def Likelihood(self, data, hypo):\n",
    "        if hypo == data:\n",
    "            return 0\n",
    "        elif hypo == 'A':\n",
    "            return 0.5 # Our p value\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "# Create a new probability mass function\n",
    "pmf = Pmf()\n",
    "\n",
    "# Our three cases A, B, and C will start with equal priors\n",
    "pmf = Monty('ABC')\n",
    "\n",
    "# We now update our pmf with the data\n",
    "pmf.Update('B')\n",
    "pmf.Print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that we use a $p$ value of 0.5, meaning that the host has an equal likelihood of choosing door B or C if the car is behind door A, we then then double our odds of winning the car to $\\frac{2}{3}$ if we switch to door C.  Feel free to go use this newfound knowledge to increase your odds of winning that car.  Unless you are this guy https://xkcd.com/1282/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 5\n",
    "### The Zombie Problem - Your Turn!\n",
    "This problem is inspired by the reddit post here: https://www.reddit.com/r/probabilitytheory/comments/500vjf/what_is_the_logic_behind_solving_this_question/ with a few modifications and clarifications.  \n",
    "\n",
    "**Problem:** ZombieLand is an amusement park gone wrong in classic Jurassic Park style.  All of the zombies have escaped, and an arbitrary number of volunteer zombie killers have been shipped in to save the remaining visitors.  Some volunteers are experienced in zombie killing and have a 2/3 probability of hitting a zombie that they shoot at.  The others are inexperienced and hit the zombies half of the time.\n",
    "\n",
    "Given a random volunteer that first hits and then misses a zombie, what is the probability that the volunteer is inexpereienced?\n",
    "\n",
    "**Solution:** Fill in the table below using the above data and hypotheses.\n",
    "\n",
    "* $H_1$: The volunteer is inexperienced\n",
    "* $H_2$: The volunteer is experienced\n",
    "* $D$: The volunteer hits a zombie then misses a zombie\n",
    "\n",
    "**Hint:** This is very close to the M&M problem above.  We have two events in our dataset.\n",
    "\n",
    "| **Hypothesis** |  **Prior Prob** | **Likelihood** | **Unnormalized Posterior** | **Posterior** |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| $H$ | $p(H)$ | $p(D|H)$ | $p(H)p(D|H)$ | $\\frac{p(H)p(D|H)}{p(D)}$ |\n",
    "| Inexperienced |  |  | |  |\n",
    "| Experienced |  |  |  |  |\n",
    "| | | | **Total Probability** | |\n",
    "|  | | | $p(D)$ | |\n",
    "| ---- | | |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 6\n",
    "### Where Am I? - The Robot Localization Problem\n",
    "Bayes' Theorem proves to be extremely useful when building mobile robots that need to know where they are within an environment at any given time.  Because of the error in motion and sensor systems, a robot's knowledge of its location in the world is based on probabilities.  Let's look at a simplified example that could feasibly be scaled up to create a working localization model.\n",
    "\n",
    "**Problem (Part A):**  We have a robot that exists within a very simple environement.  The map for this environment is a row of 6 grid cells that are colored either green or red and each labeled $x_1$, $x_2$, etc.  In real life, a larger form of this grid environment could make up what is known as an occupancy grid, or a map of the world with places that the robot can go represented as green cells and obstacles as red cells.\n",
    "\n",
    "|G|R|R|G|G|G|\n",
    "|-|-|-|-|-|-|\n",
    "|$x_1$|$x_2$|$x_3$|$x_4$|$x_5$|$x_6$|\n",
    "\n",
    "The robot has a sensor that can detect color with an 80% chance of being accurate.\n",
    "\n",
    "Given that the robot gets dropped in the environment and senses red, what is the probability of it being in each of the six locations?\n",
    "\n",
    "**Solution (Part A):**\n",
    "\n",
    "Given that the robot was dropped in a completely random cell, we can assume that the prior value for each location is equal.  If a hypothesis states that the robot is in a red cell, then given the data and the sensor error, the probability of the robot being in that cell is 80%, or the probability of correctly sensing red.  Otherwise, the probability is 20%, because the sensor would have given the wrong reading for a green cell.  We can now fill in the table.\n",
    "\n",
    "| **Hypothesis** |  **Prior Prob** | **Likelihood** | **Unnormalized Posterior** | **Posterior** |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| $H$ | $p(H)$ | $p(D|H)$ | $p(H)p(D|H)$ | $\\frac{p(H)p(D|H)}{p(D)}$ |\n",
    "| $X_1$ | $\\frac{1}{6}$ | $(20)$ | $\\frac{10}{3}$ | $\\frac{1}{12}$ |\n",
    "| $X_2$ | $\\frac{1}{6}$ | $(80)$ | $\\frac{40}{3}$ | $\\frac{1}{3}$ |\n",
    "| $X_3$ | $\\frac{1}{6}$ | $(80)$ | $\\frac{40}{3}$ | $\\frac{1}{3}$ |\n",
    "| $X_4$ | $\\frac{1}{6}$ | $(20)$ | $\\frac{10}{3}$ | $\\frac{1}{12}$ |\n",
    "| $X_5$ | $\\frac{1}{6}$ | $(20)$ | $\\frac{10}{3}$ | $\\frac{1}{12}$ |\n",
    "| $X_6$ | $\\frac{1}{6}$ | $(20)$ | $\\frac{10}{3}$ | $\\frac{1}{12}$ |\n",
    "| | | | **Total Probability** | |\n",
    "| | | | $p(D)$ | |\n",
    "| | | | $40$ | |\n",
    "\n",
    "Thus we currently have a $\\frac{1}{12}$ probability of being in a green grid cell and a $\\frac{1}{3}$ probability of being in one of the two red cells.\n",
    "\n",
    "**Problem (Part B):** This becomes an extremely useful tool as we begin to move around the map.  Let's try to get a more accurate knowledge of where the robot falls in the world by telling it to move forward one cell.\n",
    "\n",
    "The robot moves forward one cell from its previous position and the sensor reads green, again with an 80% accuracy rate.  Update the probability of the robot having started in each location.\n",
    "\n",
    "**Solution (Part B):**\n",
    "\n",
    "Now that we have some knowledge about where the robot is in environment, we can use the final probabilities from Part A as our priors.  Because this is an occupancy grid, we have the ability to know what color the grid in front of each location is.  For example, we know that $x_1$ is a green cell and $x_2$ is a red cell.  Therefore if the robot was originally in $x_1$ there is a 20% chance of it moving forward one cell and (incorrectly) reading green as the data has told us.  However, if we began at $x_3$ the robot would have an 80% chance of moving forward and reading green.  Using this logic, we can once again fill in the table.\n",
    "\n",
    "| **Hypothesis** |  **Prior Prob** | **Likelihood** | **Unnormalized Posterior** | **Posterior** |\n",
    "| :---: | :---: | :---: | :---: | :---: |\n",
    "| $H$ | $p(H)$ | $p(D|H)$ | $p(H)p(D|H)$ | $\\frac{p(H)p(D|H)}{p(D)}$ |\n",
    "| $X_1$ | $\\frac{1}{12}$ | $(20)$ | $\\frac{20}{12}$ | $\\frac{1}{33}$ |\n",
    "| $X_2$ | $\\frac{1}{3}$ | $(20)$ | $\\frac{20}{3}$ | $\\frac{4}{33}$ |\n",
    "| $X_3$ | $\\frac{1}{3}$ | $(80)$ | $\\frac{80}{3}$ | $\\frac{16}{33}$ |\n",
    "| $X_4$ | $\\frac{1}{12}$ | $(80)$ | $\\frac{80}{12}$ | $\\frac{4}{33}$ |\n",
    "| $X_5$ | $\\frac{1}{12}$ | $(80)$ | $\\frac{80}{12}$ | $\\frac{4}{33}$ |\n",
    "| $X_6$ | $\\frac{1}{12}$ | $(80)$ | $\\frac{80}{12}$ | $\\frac{4}{33}$ |\n",
    "| | | | **Total Probability** | |\n",
    "| | | | $p(D)$ | |\n",
    "| | | | $55$ | |\n",
    "\n",
    "Based on this second piece of data, even though it is prone to error, we now have a probabilistic idea of where the robot was dropped that is much greater than the original $\\frac{1}{6}$.  The robot is 16 times more likely to have moved from $x_3$ than $x_1$ and four times more likely to have moved from $x_3$ than any of the remaining cells.  Using this information we can say with $\\frac{16}{33}$ certainty that the robot moved forward from $x_3$ and is therefore now in $x_4$. \n",
    "\n",
    "**Expansion:**  This problem grows more and more applicable as you consider the many other factors that alter where the robot could be.  What if the robot doesn't always move with perfect accuracy?  What if we only know the color of some of the grid cells with a certain probability?  What if the map has millions of cells and the robot has 4 different sensors?  As you can see, this practical problem benefits from being able to implement Bayes' Theorem quickly and easily with a programming language to compute all of the data we need in order to know (probably) where a robot is in space."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
